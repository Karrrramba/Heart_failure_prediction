---
title: "README"
author: "Michal Rackiewicz"
format: gfm
editor: visual
---

This dataset contains medical records of 299 patients with previous events of heart failure.

The data was retrieved from the [UC Irvine Machine Learning Repository](https://archive.ics.uci.edu/dataset/519/heart+failure+clinical+records). More information on the data and the methodology is available in the corresponding [publication](https://www.semanticscholar.org/paper/e64579d8593140396b518682bb3a47ba246684eb).

```{r packages}
#| output: false
library(GGally)
library(gridExtra)
library(gt)
library(skimr)
library(themis)
library(tidymodels)
library(tidyverse)
library(vip)
```

```{r data}
hf <- read.csv("data/heart_failure_clinical_records_dataset.csv")
```

```{r skim}
str(hf)
skim(hf)
```

```{r target_var}
hf <- hf %>% 
  rename(death = DEATH_EVENT) %>% 
  mutate(death = as.factor(death))
```

```{r splits}
set.seed(123)
hf_split <- initial_split(hf, strata = death, prop = 0.7)
hf_train <- training(hf_split)
hf_test <- testing(hf_split)
```

```{r cv_folds}
train_folds <- vfold_cv(hf_train, v = 10, repeats = 3)
```

```{r ggpairs}
theme_set(theme_bw())

hf_train %>% 
  mutate(across(c(sex, smoking, anaemia, diabetes, high_blood_pressure), ~ as.factor(.))) %>% 
  ggpairs(switch = "y")
```

```{r eda_barplots}

facet_var = 
ggplot(hf_train, aes(as.factor(smoking), fill = death)) +
  geom_bar() +
  facet_wrap(~anaemia)
  facet_wrap(~as.factor(anaemia))
```

```{r eda_boxplots}
# function(data){
  num_vars <- hf_train %>% 
  select(where(~is.numeric(.) && max(.) > 1)) %>%
  names()

 cat_vars <- hf_train %>% 
  select(where(~is.numeric(.) && max(.) == 1)) %>% 
  names()
 
 for (c_var in cat_vars) {
   for (n_var in num_vars) {
     
     b <- ggplot(data = hf_train,
                 aes(
                   x = as.factor(!!sym(c_var)),
                   y = !!sym(n_var),
                   color = death)) +
       geom_boxplot() +
       labs(title = paste("Box plot of", n_var, "by", c_var),
            x = c_var,
            y = n_var) +
       theme_minimal()
       
     plot(b)
     
   }
 }
# }
```


## Model
```{r recipe}
rec_norm_all <- recipe(death ~ ., data = hf_train) %>% 
  step_normalize(all_predictors) 

rec_log <- recipe(death ~ ., data = hf_train) %>% 
  step_log(c(creatinine_phosphokinase, serum_creatinine)) %>% 
  step_normalize(c(platelets, age, serum_sodium, time, ejection_fraction, creatinine_phosphokinase)) %>% 
 
rec_norm_smote <- rec_norm_all %>%  
  step_smote(death)

rec_log_smote <- rec_log %>% 
  step_smote(death)

recipes <- list()
```
Check variable ranges after normalization
```{r}
hf_prep <- prep(hf_recipe)
juiced <- juice(hf_prep)
summary(juiced)

# check ratio of target variable before and after upsampling
tibble("before" = hf_train %>%
         count(death) %>%
         mutate(prop = n * 100 / sum(n)),
       "after" = juiced %>%
         count(death) %>%
         mutate(prop = n * 100 / sum(n)),
)
```

```{r model_specs}
lasso_spec <- logistic_reg(
  penalty = tune(), 
  mixture = 1,
  engine = "glmnet")

rf_spec <- rand_forest(
  trees = 100, 
  mtry = tune(), 
  min_n = tune(),
  mode = "classification",
  engine = "ranger"
)
  
svm_r_spec <- svm_rbf(
  cost = tune(),
  rbf_sigma = tune(),
  mode = "classification", 
  engine = "kernlab"
)

svm_p_spec <- svm_poly(
  cost = tune(),
  rbf_sigma = tune(),
  mode = "classification", 
  engine = "kernlab"
)
  
knn_spec <- nearest_neighbor(
  neighbors = tune(), 
  dist_power = tune(), 
  weight_func = tune(), 
  mode = "classification",
  engine = "kknn"
)
  
xg_spec <- boost_tree(
  tree_depth = tune(), 
  learn_rate = tune(), 
  loss_reduction = tune(),
  min_n = tune(), 
  sample_size = tune(), 
  trees = tune(),
  mode = "classification", 
  engine = "xgboost"
)

```


```{r workflow}
comp_models <-
  workflow_set(
    preproc = list(
      all = rec_norm_all,
      log = rec_log,
      sall = rec_norm_smote,
      slog = reclog_smote,
    ),
    models = list(ls = lasso_spec, rf = rf_spec, svm = svm_spec, xg = xg_spec),
    cross = TRUE
  )

comp_models

wf <- workflow() %>% 
  add_recipe(hf_recipe)
```

```{r}
met <- metric_set(roc_auc, mcc, precision)
```

## Lasso
```{r lasso_model}
tune_lasso <- logistic_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```

```{r lasso_tune}
lasso_grid <- tune_grid(
  add_model(wf, tune_lasso),
  resamples = train_folds,
  grid = grid_regular(
    penalty(),
    levels = 50),
  metrics = met
)

lasso_grid %>% 
  collect_metrics() %>% 
  ggplot(aes(penalty, mean, color = .metric)) +
  geom_line(size = 1.5) +
  geom_errorbar(aes(ymax = mean + std_err, 
                    ymin = mean - std_err),
                alpha = 0.5) +
  facet_wrap(~ .metric, nrow = 3, scales = "free") +
  scale_x_log10() +
  theme_bw() +
  theme(legend.position = "none")
```

Extract best-performing model
```{r}
lasso_grid %>% select_best("precision")
lasso_grid %>% select_best("roc_auc")
lasso_grid %>% select_best("mcc")

best_mcc_lasso <- lasso_grid %>% select_best("mcc")

```

```{r}
final_lasso <- finalize_workflow(wf %>% add_model(tune_lasso), 
                                   best_mcc_lasso)
```

```{r}
last_fit(final_lasso, hf_split, metrics = met) %>% 
  collect_metrics()
```

### Variable importance
```{r}
final_lasso %>% 
  fit(hf_train) %>% 
  extract_fit_parsnip() %>% 
  vip::vi(lambda = best_roc_lasso$penalty) %>% 
  mutate(Variable = fct_reorder(Variable, Importance)) %>% 
  ggplot(aes(Importance, Variable, color = Sign)) +
  geom_point(size = 3) +
  labs(y = NULL)
```

## Random forest
```{r rf_model}
tune_rf <- rand_forest(
  mode = "classification",
  engine = "ranger",
  trees = 100, 
  mtry = tune(), 
  min_n = tune()
)
```

```{r rf_tune}
rf_grid <- tune_grid(
  add_model(wf, tune_rf),
  resamples = train_folds,
  grid = grid_regular(
    mtry(range = c(3, 8)),
    min_n(),
    levels = 50
  )
)
```

```{r}
rf_grid %>% select_best("roc_auc")
rf_grid %>% select_best("precision")
rf_grid %>% select_best("mcc")

best_roc_rf <- rf_grid %>% select_best("mcc")
```

```{r}
final_rf <- finalize_workflow(wf, add_model(tune_rf),
                              best_roc_rf)
```

```{r}
last_fit(final_rf, hf_split, metrics = met) %>% 
  collect_metrics()
```

## Support vector machines
```{r}
tune_svm <- svm_rbf(
  mode = "classification", 
  engine = "kernlab",
  cost = tune(),
  rbf_sigma = tune()
)
```

```{r}
svm_grid <- tune_grid(
  add_model(wf, tune_svm),
  resamples = train_folds,
  grid = grid_regular(
    cost(),
    rbf_sigma(),
    levels = 50
  )
)
```

```{r}
svm_grid %>% select_best("roc_auc")
svm_grid %>% select_best("precision")
svm_grid %>% select_best("mcc")

# best_roc_svm <- svm_grid %>% select_best("roc_auc")
```

```{r}
final_svm <- finalize_workflow(wf %>% add_model(tune_svm),
                               best_roc_svm)
```

```{r}
last_fit(final_svm, hf_split, metrics = met) %>% 
  collect_metrics()
```

## Xgboost
```{r}
tune_xgb <- xgb
```

